---
title: "Bonus Homework"
author: "Safak Kilic - IE582 - Fall 2018"
---

```{r setupfirst, echo=FALSE, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

set.seed(10)

require(MASS)
require(randomForest)
require(cluster)
require(data.table)
```

```{r setup, echo=FALSE}

# this function extracts mean and covariance information per-cluster from clustered data
findparams <- function(data = dists$full[,-ncol(dists$full)], clustering, nclus) {
  clusts <- as.data.table(cbind(data, predcluster = clustering))
  params <- list()
  for (i in 1:ndists) {
    ith_clust <- copy(clusts)[predcluster == i][, predcluster := NULL]
    params[[i]] <- list(means = as.numeric(sapply(ith_clust, mean)), 
                        covmat = cov(ith_clust),
                        i = i)
  }
  return(params)
}

# for some cluster mean results, finds the mean result in the reference table which is closest 
findclosest <- function(checktable, reftable) {
  checktable <- as.data.frame(checktable)
  reftable <- as.data.frame(reftable)
  closestvector <- apply(checktable, 1, function(x) which.min(dist(rbind(x, reftable))))
  return(closestvector)
}

```  

## Generating Data

The following code will be used to generate our multivariate normally distributed data. Due to the data being 8-dimensional, it is quite hard to visualize and decide on mean/covariance parameters by inspection. Therefore the code generates random data making sure the covariance matrix is positive definite (which is necessary for the mvrnorm function).

```{r generating}
# this function generates <ndists> normally distributed clusters
generatedists <- function(ndists, meanrange, covmax, dims, noise = FALSE) {
  mvns <- list()
  params <- list()
  for(i in 1:ndists) {
    # uniform random means
    mn <- runif(dims, min(meanrange), max(meanrange))
    
    # produce a covariance matrix thats positive definite by "dot producting" uniform random
    # matrix with itself
    mat <- matrix(runif(dims^2, min = -covmax, max = covmax), ncol = dims, nrow = dims) 
    matpd <- t(mat) %*% mat
    
    mvns[[i]] <- cbind(mvrnorm(500, mn, matpd), i)
    params[[i]] <- list(i = i, means = mn, covmat = matpd)
  }
  concmvn <- do.call("rbind", mvns)
  
  # if Bernoulli noise should be added, this section forms a noise matrix of size <dims>*<number of data>
  if(noise) {
    noisemat <- matrix(nrow = 500*ndists, ncol = dims)
    for(i in 1:dims) {
      # Bernoulli noise between 0 and 1 is generated and scaled to 5% characteristic data range
      noisemat[,i] <- (rbinom(500*ndists, 1, 0.5)/1 - 0.5)*(max(meanrange)-min(meanrange))/20
    }
    concmvn[,-ncol(concmvn)] <- concmvn[,-ncol(concmvn)] + noisemat
  }
  
  return(list(full = concmvn, mvns = mvns, params = params))
}

rng <- 4 # this range of means SEVERELY affects how well algorithms will perform
ndists <- 4
meanrange <- c(-rng, rng)
dists <- generatedists(ndists, meanrange, 1, 8)
# store means
meanstbl <- cbind(as.data.table(t(sapply(dists$params, function(x) c(x$means, distname = x$i)))), from = rep("generated", ndists), closest = 1:ndists)
```  

We can take a quick look at the top of the data we have generated, and the means of each distribution.
```{r look1, echo = FALSE}
head(dists$full)
meanstbl
```  

## Random Forest Dissimilarity

Constructing the random forest and computing dissimilarity is trivial:

```{r rf}
forest <- randomForest(dists$full[,-ncol(dists$full)], keep.forest=FALSE, proximity=TRUE)
forest$dissimilarity <- as.dist(1 - forest$proximity)
```  

## Partitioning Around Medoids (PAM) & Hierarchical Clustering

These two methods use the dissimilarity information from the random forest.

```{r pamhclus, echo= FALSE, warning=FALSE}
pam <- pam(forest$dissimilarity, ndists, diss = TRUE)
print("PAM Clustering Results")
table(pam$clustering, dists$full[,ncol(dists$full)])
pamparams <- findparams(dists$full[,-ncol(dists$full)], pam$clustering, ndists)
pammeanstbl <- cbind(as.data.table(t(sapply(pamparams, function(x) c(x$means, distname = x$i)))), from = rep("PAM", ndists))
pammeanstbl <- cbind(pammeanstbl, closest = findclosest(copy(pammeanstbl)[, c("distname", "from", "closest") := NULL], copy(meanstbl)[, c("distname", "from", "closest") := NULL]))

hclus <- hclust(forest$dissimilarity, method = "ward.D2")
hclusters <- cutree(hclus, k=4)
print("Hierarchical Clustering Results")
table(hclusters, dists$full[,ncol(dists$full)])
hparams <- findparams(dists$full[,-ncol(dists$full)], hclusters, ndists)
hmeanstbl <- cbind(as.data.table(t(sapply(hparams, function(x) c(x$means, distname = x$i)))), from = rep("Hclust", ndists))
hmeanstbl <- cbind(hmeanstbl, closest = findclosest(copy(hmeanstbl)[, c("distname", "from", "closest") := NULL], copy(meanstbl)[, c("distname", "from", "closest") := NULL]))
```  

As one can see, these methods show fair recall for clusters at the given possible mean range [-4, 4] and covariance parameter of 1. That is, each clustering algorithm gathers between 300-400 data points from the same distribution within the same cluster, varying sharply both with range of mean and with iterated generations. The former effect is predictable, but is obfuscated by the unpredictability of the latter effect. The algorithms both have poor precision, however, and usually end up with one larger cluster containing significant parts of data from other distributions (around 100-200).

## K-means Clustering

The K-means method works independently of the random forest distance calculation. 

```{r kmclus, echo= FALSE, warning=FALSE}
kmn <- kmeans(dists$full[,-ncol(dists$full)], centers = 4, nstart = 2)
print("K-Means Clustering Results")
table(kmn$cluster, dists$full[,ncol(dists$full)])
kmnparams <- findparams(dists$full[,-ncol(dists$full)], kmn$cluster, ndists)
kmnmeanstbl <- cbind(as.data.table(t(sapply(kmnparams, function(x) c(x$means, distname = x$i)))), from = rep("K means", ndists))
kmnmeanstbl <- cbind(kmnmeanstbl, closest = findclosest(copy(kmnmeanstbl)[, c("distname", "from", "closest") := NULL], copy(meanstbl)[, c("distname", "from", "closest") := NULL]))
```  

The K-means algorithm in particular has very good precision compared to the random forest methods, and is mostly unaffected by repeating the experiment. It will usually gather upwards of 450 data points from each distribution in a single cluster, with most runs having at least three clusters which contain more than 480 data points from a single distribution. This shortcoming of random forests is probably simply caused by the fact that random partitioning can miss good split boundaries. 

## Comparing Generation Parameters to Cluster Parameters

The means from data generation are compared to those from clustering in the table below: 

```{r meancomp, echo =FALSE}
comparemeanstbl <- rbind(meanstbl, pammeanstbl, hmeanstbl, kmnmeanstbl)
print("Comparison of Means")
comparemeanstbl
```  

The "closest" column indicates which distribution from the original generation is closest to this specific cluster from this particular clustering method.

The means comparison shows that the found clusters estimate where the clusters are rather well - i.e. there is rarely a case when finding which distribution a mean is closest to is difficult. The covariance matrices have also been provided for your convenience, however they are not easy to compare by inspection.

```{r covcomp, echo=FALSE}
print("Comparison of Covariances")
covlist <- lapply(dists$params, function(x) x$covmat)
print("Original Cov. Matrix")
covlist

pamcovlist <- lapply(pamparams, function(x) x$covmat)
print("PAM Cov. Matrix")
print(paste("Refers to dists. in order of", paste(pammeanstbl$closest, collapse = " ")))
pamcovlist

hcovlist <- lapply(hparams, function(x) x$covmat)
print("Hier. Clust. Cov. Matrix")
print(paste("Refers to dists. in order of", paste(hmeanstbl$closest, collapse = " ")))
hcovlist

kmncovlist <- lapply(kmnparams, function(x) x$covmat)
print("K-means Cov. Matrix")
print(paste("Refers to dists. in order of", paste(kmnmeanstbl$closest, collapse = " ")))
kmncovlist
```

## Additional Noise & Discussion

As seen above, the method for generating multivariate normal distribution has an optional Bernoulli noise parameter which adds the result of a (distribution mean-scaled) Bernoulli process to each variable in the distribution. Below are the clustering results with noise added:

```{r noise}
rng <- 4 # this range of means SEVERELY affects how well algorithms will perform
ndists <- 4
meanrange <- c(-rng, rng)
dists <- generatedists(ndists, meanrange, 1, 8, noise = TRUE)
# store means
meanstbl <- cbind(as.data.table(t(sapply(dists$params, function(x) c(x$means, distname = x$i)))), from = rep("generated", ndists), closest = 1:ndists)
head(dists$full)
meanstbl
```  
```{r pamhcluskmnnoise, echo= FALSE, warning=FALSE}
forest <- randomForest(dists$full[,-ncol(dists$full)], keep.forest=FALSE, proximity=TRUE)
forest$dissimilarity <- as.dist(1 - forest$proximity)
pam <- pam(forest$dissimilarity, ndists, diss = TRUE)
print("PAM Clustering Results")
table(pam$clustering, dists$full[,ncol(dists$full)])
pamparams <- findparams(dists$full[,-ncol(dists$full)], pam$clustering, ndists)
pammeanstbl <- cbind(as.data.table(t(sapply(pamparams, function(x) c(x$means, distname = x$i)))), from = rep("PAM", ndists))
pammeanstbl <- cbind(pammeanstbl, closest = findclosest(copy(pammeanstbl)[, c("distname", "from", "closest") := NULL], copy(meanstbl)[, c("distname", "from", "closest") := NULL]))

hclus <- hclust(forest$dissimilarity, method = "ward.D2")
hclusters <- cutree(hclus, k=4)
print("Hierarchical Clustering Results")
table(hclusters, dists$full[,ncol(dists$full)])
hparams <- findparams(dists$full[,-ncol(dists$full)], hclusters, ndists)
hmeanstbl <- cbind(as.data.table(t(sapply(hparams, function(x) c(x$means, distname = x$i)))), from = rep("Hclust", ndists))
hmeanstbl <- cbind(hmeanstbl, closest = findclosest(copy(hmeanstbl)[, c("distname", "from", "closest") := NULL], copy(meanstbl)[, c("distname", "from", "closest") := NULL]))

kmn <- kmeans(dists$full[,-ncol(dists$full)], centers = 4, nstart = 2)
print("K-Means Clustering Results")
table(kmn$cluster, dists$full[,ncol(dists$full)])
kmnparams <- findparams(dists$full[,-ncol(dists$full)], kmn$cluster, ndists)
kmnmeanstbl <- cbind(as.data.table(t(sapply(kmnparams, function(x) c(x$means, distname = x$i)))), from = rep("K means", ndists))
kmnmeanstbl <- cbind(kmnmeanstbl, closest = findclosest(copy(kmnmeanstbl)[, c("distname", "from", "closest") := NULL], copy(meanstbl)[, c("distname", "from", "closest") := NULL]))
```  

Theoretically, because random forests can be said to model Gaussian distribution, one can expect them to perform well when clusters behave roughly in a Gaussian manner. When Bernoulli noise is added, random forest-based models can thus be expected to fail. The results seem to bear this out, with the K-means algorithm still being much more successful and the forest-based models doing somewhat worse, but since the clustering is already quite sensitive to the generation process (especially with high dimensions), it is difficult to make this remark conclusively. 

Finally, we can mention that growing the number of dimensions of the problem will likely see the random forest-based methods to improve in performance compared to K-means methods, since the typical "distance" between data points will increase, making proximities much more specific. 

## Appendix

```{r append, eval = FALSE}
require(MASS)
require(randomForest)
require(cluster)
require(data.table)
#### necess fctns ####
# this function extracts mean and covariance information per-cluster from clustered data
findparams <- function(data = dists$full[,-ncol(dists$full)], clustering, nclus) {
  clusts <- as.data.table(cbind(data, predcluster = clustering))
  params <- list()
  for (i in 1:ndists) {
    ith_clust <- copy(clusts)[predcluster == i][, predcluster := NULL]
    params[[i]] <- list(means = as.numeric(sapply(ith_clust, mean)), 
                        covmat = cov(ith_clust),
                        i = i)
  }
  return(params)
}
# this function generates <ndists> normally distributed clusters
generatedists <- function(ndists, meanrange, covmax, dims, noise = FALSE) {
  mvns <- list()
  params <- list()
  for(i in 1:ndists) {
    # uniform random means
    mn <- runif(dims, min(meanrange), max(meanrange))
    
    # produce a covariance matrix thats positive definite by "dotting" uniform random
    # matrix with itself
    mat <- matrix(runif(dims^2, min = -covmax, max = covmax), ncol = dims, nrow = dims) 
    matpd <- t(mat) %*% mat
    
    mvns[[i]] <- cbind(mvrnorm(500, mn, matpd), i)
    params[[i]] <- list(i = i, means = mn, covmat = matpd)
  }
  concmvn <- do.call("rbind", mvns)
  
  # if Bernoulli noise should be added, this section forms a noise matrix of size <dims>*<number of data>
  if(noise) {
    noisemat <- matrix(nrow = 500*ndists, ncol = dims)
    for(i in 1:dims) {
      # Bernoulli noise between 0 and 1 is generated and scaled to 5% characteristic data range
      noisemat[,i] <- (rbinom(500*ndists, 1, 0.5)/1 - 0.5)*(max(meanrange)-min(meanrange))/20
    }
    concmvn[,-ncol(concmvn)] <- concmvn[,-ncol(concmvn)] + noisemat
  }
  
  return(list(full = concmvn, mvns = mvns, params = params))
}

# for some cluster mean results, finds the mean result in the reference table which is closest 
findclosest <- function(checktable, reftable) {
  checktable <- as.data.frame(checktable)
  reftable <- as.data.frame(reftable)
  closestvector <- apply(checktable, 1, function(x) which.min(dist(rbind(x, reftable))))
  return(closestvector)
}


#### generate distributions ####
rng <- 4 # this range of means SEVERELY affects how well algorithms will perform
ndists <- 4
meanrange <- c(-rng, rng)
dists <- generatedists(ndists, meanrange, 1, 8, noise = FALSE)
# store means
meanstbl <- cbind(as.data.table(t(sapply(dists$params, function(x) c(x$means, distname = x$i)))), from = rep("generated", ndists), closest = 1:ndists)

#### test to see if dists can be told apart ####
testmatrix <- matrix(ncol = ndists, nrow = ndists)
for (i in 1:ndists) {
  for (j in 1:ndists) {
    testmatrix[i,j] <- ks.test(dists$mvns[[i]][,-ncol(dists$mvns[[i]])], dists$mvns[[j]][,-ncol(dists$mvns[[j]])])$p.value
  }
}
plot(dists$full[,-ncol(dists$full)], col=factor(dists$full[,ncol(dists$full)]))

#### construct RF ####
forest <- randomForest(dists$full[,-ncol(dists$full)], keep.forest=FALSE, proximity=TRUE)
forest$dissimilarity <- as.dist(1 - forest$proximity)

#### PAM ####
pam <- pam(forest$dissimilarity, ndists, diss = TRUE)
print("PAM Clustering Results")
table(pam$clustering, dists$full[,ncol(dists$full)])
pamparams <- findparams(dists$full[,-ncol(dists$full)], pam$clustering, ndists)
pammeanstbl <- cbind(as.data.table(t(sapply(pamparams, function(x) c(x$means, distname = x$i)))), from = rep("PAM", ndists))
pammeanstbl <- cbind(pammeanstbl, closest = findclosest(copy(pammeanstbl)[, c("distname", "from", "closest") := NULL], copy(meanstbl)[, c("distname", "from", "closest") := NULL]))

#### Hclust ####
hclus <- hclust(forest$dissimilarity, method = "ward.D2")
hclusters <- cutree(hclus, k=4)
print("Hierarchical Clustering Results")
table(hclusters, dists$full[,ncol(dists$full)])
hparams <- findparams(dists$full[,-ncol(dists$full)], hclusters, ndists)
hmeanstbl <- cbind(as.data.table(t(sapply(hparams, function(x) c(x$means, distname = x$i)))), from = rep("Hclust", ndists))
hmeanstbl <- cbind(hmeanstbl, closest = findclosest(copy(hmeanstbl)[, c("distname", "from", "closest") := NULL], copy(meanstbl)[, c("distname", "from", "closest") := NULL]))

#### K means ####
kmn <- kmeans(dists$full[,-ncol(dists$full)], centers = 4, nstart = 2)
print("K-Means Clustering Results")
table(kmn$cluster, dists$full[,ncol(dists$full)])
kmnparams <- findparams(dists$full[,-ncol(dists$full)], kmn$cluster, ndists)
kmnmeanstbl <- cbind(as.data.table(t(sapply(kmnparams, function(x) c(x$means, distname = x$i)))), from = rep("K means", ndists))
kmnmeanstbl <- cbind(kmnmeanstbl, closest = findclosest(copy(kmnmeanstbl)[, c("distname", "from", "closest") := NULL], copy(meanstbl)[, c("distname", "from", "closest") := NULL]))

comparemeanstbl <- rbind(meanstbl, pammeanstbl, hmeanstbl, kmnmeanstbl)
print("Comparison of Means")
comparemeanstbl

print("Comparison of Covariances")
covlist <- lapply(dists$params, function(x) x$covmat)
print("Original Cov. Matrix")
covlist

pamcovlist <- lapply(pamparams, function(x) x$covmat)
print("PAM Cov. Matrix")
print(paste("Refers to dists. in order of", paste(pammeanstbl$closest, collapse = " ")))
pamcovlist

hcovlist <- lapply(hparams, function(x) x$covmat)
print("Hier. Clust. Cov. Matrix")
print(paste("Refers to dists. in order of", paste(hmeanstbl$closest, collapse = " ")))
hcovlist

kmncovlist <- lapply(kmnparams, function(x) x$covmat)
print("K-means Cov. Matrix")
print(paste("Refers to dists. in order of", paste(kmnmeanstbl$closest, collapse = " ")))
kmncovlist
```

